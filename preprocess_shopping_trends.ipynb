{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed46be1f",
   "metadata": {},
   "source": [
    "# Shopping Trends Data Preprocessing\n",
    "\n",
    "This notebook walks through the preprocessing steps for the `shopping_trends.csv` dataset, preparing it for clustering and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c5fa3a",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "We use pandas for data manipulation, numpy for numerical operations, and scikit-learn for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f99d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7ee66",
   "metadata": {},
   "source": [
    "## Step 2: Load the Dataset\n",
    "\n",
    "Read the CSV file into a pandas DataFrame and display the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01c31f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('shopping_trends.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da23edaa",
   "metadata": {},
   "source": [
    "## Step 3: Check for Missing Values\n",
    "\n",
    "Missing values can affect analysis and modeling. We check for any null values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20331a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing Values:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a57f387",
   "metadata": {},
   "source": [
    "## Step 4: Drop Irrelevant Columns\n",
    "\n",
    "Columns like `Customer ID` are identifiers and do not contribute to clustering. We remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84bdd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Customer ID'], errors='ignore')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d40337e",
   "metadata": {},
   "source": [
    "## Step 5: Handle Duplicate Rows\n",
    "\n",
    "Duplicate rows can bias analysis. We check for and remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d235427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of duplicate rows:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe5fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad5d8bb",
   "metadata": {},
   "source": [
    "## Step 6: Identify Numerical and Categorical Columns\n",
    "\n",
    "We separate columns into numerical and categorical types for appropriate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['Age', 'Purchase Amount (USD)', 'Review Rating', 'Previous Purchases']\n",
    "\n",
    "categorical_cols = ['Gender', 'Item Purchased', 'Category', 'Location', 'Size', 'Color', \n",
    "                    'Season', 'Subscription Status', 'Payment Method', 'Shipping Type', \n",
    "                    'Discount Applied', 'Promo Code Used', 'Preferred Payment Method', \n",
    "                    'Frequency of Purchases']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f6d327",
   "metadata": {},
   "source": [
    "## Step 7: Detect Outliers in Numerical Columns\n",
    "\n",
    "Outliers can skew results. We use the Interquartile Range (IQR) method to identify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c39f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "    print(f\"Outliers in {col}:\\n\", outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81d17f4",
   "metadata": {},
   "source": [
    "## Step 8: Cap Outliers (Optional)\n",
    "\n",
    "To reduce the impact of extreme values, we cap `Purchase Amount (USD)` at its 95th percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de06a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Purchase Amount (USD)'] = df['Purchase Amount (USD)'].clip(upper=df['Purchase Amount (USD)'].quantile(0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa6b5a",
   "metadata": {},
   "source": [
    "## Step 9: Preprocessing Pipeline\n",
    "\n",
    "We use scikit-learn's `ColumnTransformer` to scale numerical features and one-hot encode categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7568435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c173588c",
   "metadata": {},
   "source": [
    "## Step 10: Apply Preprocessing\n",
    "\n",
    "Fit and transform the data using the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988aa16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessor.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa114e19",
   "metadata": {},
   "source": [
    "## Step 11: Retrieve Feature Names\n",
    "\n",
    "After one-hot encoding, we get the new feature names for the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a6a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
    "feature_names = numerical_cols + list(cat_feature_names)\n",
    "X = pd.DataFrame(X, columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0daa95",
   "metadata": {},
   "source": [
    "## Step 12: Save Preprocessed Data\n",
    "\n",
    "Export the cleaned and transformed dataset to a new CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f1996",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('preprocessed_shopping_trends.csv', index=False)\n",
    "print(\"Preprocessed data saved to 'preprocessed_shopping_trends.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b1311",
   "metadata": {},
   "source": [
    "## Step 13: Preview Preprocessed Data\n",
    "\n",
    "Display the first few rows of the final preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cf8d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessed Data (first 5 rows):\\n\", X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c7b743",
   "metadata": {},
   "source": [
    "## Summary of Preprocessing Steps\n",
    "\n",
    "- **Missing Values:** Checked for nulls; handle as needed.\n",
    "- **Duplicates:** Removed duplicate rows.\n",
    "- **Outliers:** Detected using IQR; capped extreme values.\n",
    "- **Encoding:** Categorical variables one-hot encoded.\n",
    "- **Scaling:** Numerical features standardized.\n",
    "- **Irrelevant Columns:** Dropped identifiers.\n",
    "- **Saving:** Preprocessed data exported for further use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
